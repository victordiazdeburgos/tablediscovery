{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# YAKE\n",
    "import yake\n",
    "\n",
    "# Word embeddings\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    set1 = set(set1)\n",
    "    set2 = set(set2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the jaccard similarity of two tables with respect to the values of their columns\n",
    "def column_similarity(tab1, tab2):\n",
    "    numColsTab1 = tab1.shape[1]\n",
    "    numColsTab2 = tab2.shape[1]\n",
    "\n",
    "    jaccardSimCols = np.zeros((numColsTab1, numColsTab2))\n",
    "    for i in range(numColsTab1):\n",
    "        # For each column in Table 1 we compute the Jaccard similarity with each column of Table 2\n",
    "        col1 = tab1.iloc[:,i]\n",
    "        for j in range(numColsTab2):\n",
    "            col2 = tab2.iloc[:,j]\n",
    "            jaccardSimCols[i,j] = jaccard_similarity(col1, col2)\n",
    "\n",
    "    similarity = jaccardSimCols.mean() # The simlarity of the tables is the mean of the Jaccard similarities computed pairwise\n",
    "    return similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the jaccard similarity of two tables with respect to the values of their rows\n",
    "def row_similarity(tab1, tab2):\n",
    "    numRowsTab1 = tab1.shape[0]\n",
    "    numRowsTab2 = tab2.shape[0]\n",
    "\n",
    "    jaccardSimRows = np.zeros((numRowsTab1, numRowsTab2))\n",
    "    for i in range(numRowsTab1):\n",
    "        # For each row in Table 1 we compute the Jaccard similarity with each row in Table 2\n",
    "        row1 = tab1.iloc[i,:]\n",
    "        for j in range(numRowsTab2):\n",
    "            row2 = tab2.iloc[j,:]\n",
    "            jaccardSimRows[i,j] = jaccard_similarity(row1, row2)\n",
    "\n",
    "    similarity = jaccardSimRows.mean() # The simlarity of the tables is the mean of the Jaccard similarities computed pairwise\n",
    "    return similarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform simple similarity\n",
    "def rank_simple_similarity(input_table, data_lake, by='column', k=10):\n",
    "    numTables = len(data_lake) # number of tables in the data lake\n",
    "    similarities = np.zeros(numTables)\n",
    "    for i, table in enumerate(data_lake):\n",
    "        if by == 'column':\n",
    "            similarities[i] = column_similarity(input_table, table)\n",
    "        elif by == 'row':\n",
    "            # This option was running over 21 hours and it did not finish\n",
    "            similarities[i] = row_similarity(input_table, table)\n",
    "        else:\n",
    "            print(\"The 'by' argument has to be either 'column' or 'row'. {} is not accepted\".format(by))\n",
    "            return []\n",
    "        # print(\"Similarity with Table {}: {}\".format(i,similarities[i]))\n",
    "  \n",
    "    ranked_tables = np.argsort(similarities)[::-1][:k] # indexes of the top-k tables sorted by similarity\n",
    "    ranked_similarites = np.sort(similarities)[::-1][:k] # similarity of the top-k tables sorted by similarity\n",
    "\n",
    "    return ranked_tables, ranked_similarites\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_lda(text, numKeywords = 10):\n",
    "    # Function to extract keywords from text using LDA\n",
    "\n",
    "    # Document-Term Matrix\n",
    "    vectorizer = CountVectorizer()\n",
    "    dtm = vectorizer.fit_transform([text])\n",
    "\n",
    "    # LDA\n",
    "    num_topics = 1  # Number of topics to identify\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics)\n",
    "    lda_model.fit(dtm)\n",
    "\n",
    "    # Interpret the topic\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topic = lda_model.components_[0]\n",
    "    top_words_indices = topic.argsort()[:-numKeywords - 1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_indices]\n",
    "    top_weights = [topic[i] for i in top_words_indices]\n",
    "\n",
    "    # Normalize the weights        ------------------------------------------------ Try without normalizing. These weights represent the importance on the overall table, not only with respect to the other keywords  --------------------------------------\n",
    "    total_weight = sum(top_weights)\n",
    "    normalized_weights = [weight / total_weight for weight in top_weights]\n",
    "\n",
    "    # Store the top words and normalized weights in variables\n",
    "    topic_words_and_weights = list(zip(top_words, normalized_weights))\n",
    "\n",
    "    return topic_words_and_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_yake(text, numKeywords = 10, lenNGram = 1):\n",
    "    # Function to extract keywords from text using Yake\n",
    "    \n",
    "    kw_extractor = yake.KeywordExtractor(n=lenNGram, top=numKeywords)\n",
    "    keywords= kw_extractor.extract_keywords(text)\n",
    "    keywords = sorted(keywords, key = lambda x: x[1], reverse = True)\n",
    "    return keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_jaccard_similarity(keywords1, weights1, keywords2, weights2):\n",
    "    # Function to compute the weighted Jaccard similarity between two sets\n",
    "    \n",
    "    intersection = keywords1.intersection(keywords2)\n",
    "    union = keywords1.union(keywords2)\n",
    "\n",
    "    numerator = sum(min(weights1[word], weights2[word]) for word in intersection)\n",
    "    denominator = sum(max(weights1.get(word, 0), weights2.get(word, 0)) for word in union)\n",
    "\n",
    "    similarity = numerator / denominator\n",
    "    return similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction (LDA/YAKE) + Weighted Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keywords(text, method):\n",
    "    # Funtion to call the correct keyword extraction method\n",
    "    if method == \"lda\":\n",
    "        keywords = keywords_lda(text)\n",
    "    elif method == \"yake\":\n",
    "        keywords = keywords_yake(text)\n",
    "    else:\n",
    "        raise ValueError(\"The method to extract the keywords has to be either 'lda' or 'yake'. '{}' is not accepted\".format(method))\n",
    "    \n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform table discovery based on the similarity of the keywords of the tables\n",
    "def rank_keywords(input_table_text, data_lake, keywordExtractionMethod, k=10):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    numTables = len(data_lake) # number of tables in the data lake\n",
    "\n",
    "    # Find keywords of the input table and their corresponding weights\n",
    "    input_table_words_topic = find_keywords(input_table_text, keywordExtractionMethod)\n",
    "    input_table_words = set([t[0] for t in input_table_words_topic])\n",
    "    input_table_weights = {t[0]: t[1] for t in input_table_words_topic}\n",
    "\n",
    "    running_time = time.time() - start_time\n",
    "\n",
    "    similarities = np.zeros(len(data_lake))\n",
    "    for i, table_text in enumerate(data_lake):\n",
    "        # Find keywords of the table and their corresponding weights\n",
    "        table_words_topic = find_keywords(table_text, keywordExtractionMethod)\n",
    "        table_words = set([t[0] for t in table_words_topic])\n",
    "        table_weights = {t[0]: t[1] for t in table_words_topic}\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute Weighted Jaccard Similarity between input table and table from the data lake\n",
    "        similarities[i] = weighted_jaccard_similarity(input_table_words, input_table_weights, table_words, table_weights)\n",
    "        # print(\"Similarity with Table {}: {}\".format(i,similarities[i]))\n",
    "\n",
    "        running_time += time.time() - start_time\n",
    "\n",
    "    if k > numTables:\n",
    "        print(\"The introduced k (k = {}) is larger than the number of tables in the data lake. The output is the ranking of all tables in the data lake.\".format(k))\n",
    "        k = numTables # We rank all tables in the data lake\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ranked_tables = np.argsort(similarities)[::-1][:k] # indexes of the top-k tables sorted by similarity\n",
    "    ranked_similarites = np.sort(similarities)[::-1][:k] # similarity of the top-k tables sorted by similarity\n",
    "\n",
    "    running_time += time.time() - start_time\n",
    "\n",
    "    return ranked_tables, ranked_similarites, running_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword extraction + Word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_word2vec(keywords, weights, model):\n",
    "    # Funtion to transforms keywords to embeddings using Word2Vec\n",
    "    embeddings = []\n",
    "    weights_filtered = []\n",
    "    for word, weight in zip(keywords, weights):\n",
    "        if word in model:\n",
    "            embeddings.append(model[word])\n",
    "            weights_filtered.append(weight)\n",
    "        # else:\n",
    "        #     print(\"'{}' does not have a predefined embedding.\".format(word))\n",
    "\n",
    "    return list(tuple(zip(embeddings, weights_filtered)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_bert(keywords, model, tokenizer):\n",
    "    # Initialize a list to store the embeddings\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Process each keyword to obtain the embeddings\n",
    "    for keyword in keywords:\n",
    "        \n",
    "        # Add the special tokens.\n",
    "        marked_text = \"[CLS] \" + keyword + \" [SEP]\"\n",
    "\n",
    "        # Tokenize the keyword\n",
    "        tokens = tokenizer.tokenize(marked_text)\n",
    "      \n",
    "        # Convert tokens to token IDs\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Mark each of the tokens as belonging to sentence \"1\" because, in our case, all tokens belong to the same keywords. \n",
    "        segments_ids = [1] * len(tokens)\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([token_ids])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "        # Predict hidden states features for each layer\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "        token_vecs = encoded_layers[11][0]\n",
    "\n",
    "        keyword_embedding = torch.mean(token_vecs, dim=0)\n",
    "        \n",
    "        # Append the embedding to the embeddings list\n",
    "        embeddings_list.append(keyword_embedding.numpy())\n",
    "\n",
    "    return embeddings_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_centroid_cosine_similarity(embeddings1, weights1, embeddings2, weights2):\n",
    "    # Function to compute the cosine similarity between two weighted embedding sets. We first find the centroid of the sets and then compute the cosine similarity between them.\n",
    "\n",
    "    weighted_centroid1 = np.average(embeddings1, axis=0, weights=weights1)\n",
    "    weighted_centroid2 = np.average(embeddings2, axis=0, weights=weights2)\n",
    "\n",
    "    cosine_sim = cosine_similarity(weighted_centroid1.reshape(1, -1), weighted_centroid2.reshape(1, -1))\n",
    "\n",
    "    return cosine_sim[0, 0]\n",
    "\n",
    "\n",
    "def weighted_pairwise_cosine_similarity(embeddings1, weights1, embeddings2, weights2, i, aggregation='mean'):\n",
    "    # Function to compute the cosine similarity between two weighted embedding sets. We first compute the similarites pairwise and then we aggregate them.\n",
    "    \n",
    "    pairwise_similarities = cosine_similarity(embeddings1, embeddings2)\n",
    "    weighted_pairwise_similarities = pairwise_similarities * np.outer(weights1, weights2)\n",
    "\n",
    "    if aggregation == 'mean':\n",
    "        similarity = np.mean(weighted_pairwise_similarities)\n",
    "    elif aggregation == 'max':\n",
    "        similarity = np.max(weighted_pairwise_similarities)\n",
    "    elif aggregation == 'sum':\n",
    "        similarity = np.sum(weighted_pairwise_similarities)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction (LDA/YAKE) + Word Embeddings (Word2Vec/BERT) + Weighted Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_embeddings(text, methodKeywords, methodEmbeddings, model, tokenizer = None):\n",
    "    # Function to find the embeddings of the keywords of a given table\n",
    "\n",
    "    keywords_weights = find_keywords(text, methodKeywords)\n",
    "    keywords = [t[0] for t in keywords_weights]\n",
    "    weights = [t[1] for t in keywords_weights]\n",
    "\n",
    "    if methodEmbeddings == \"word2vec\":\n",
    "        embeddings_weights = embeddings_word2vec(keywords, weights, model) # returns embeddings and corresponding weight\n",
    "        embeddings = np.array([i[0] for i in embeddings_weights])\n",
    "        new_weights = np.array([i[1] for i in embeddings_weights])\n",
    "    elif methodEmbeddings == \"bert\":\n",
    "        embeddings = embeddings_bert(keywords, model, tokenizer) # returns embeddings\n",
    "        embeddings = np.array(embeddings)\n",
    "        new_weights = np.array(weights) # weights do not change using Bert\n",
    "    else:\n",
    "        raise ValueError(\"The method to extract the keywords has to be either 'word2vec' or 'bert'. '{}' is not accepted\".format(methodEmbeddings))\n",
    "    \n",
    "    return embeddings, new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform table discovery based on the similarity of the embeddings of the keywords of the tables\n",
    "def rank_embeddings(input_table_text, data_lake, keywordExtractionMethod, embeddingsMethod, model, tokenizer = None, k=10):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    numTables = len(data_lake) # number of tables in the data lake\n",
    "\n",
    "    # Find embeddings of the keywords of the input table\n",
    "    embeddings_input, weights_input = find_embeddings(input_table_text, keywordExtractionMethod, embeddingsMethod, model, tokenizer)\n",
    "\n",
    "    running_time = time.time() - start_time\n",
    "\n",
    "    similarities = np.zeros(numTables)\n",
    "    for i, table_text in enumerate(data_lake):\n",
    "        # Find embeddings of the keywords of the table of the data lake\n",
    "        embeddings_table, weights_table = find_embeddings(table_text, keywordExtractionMethod, embeddingsMethod, model, tokenizer)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # If the embeddings model has found any embedding for the table\n",
    "        if np.size(embeddings_table) > 0:\n",
    "            # Compute similarity between input table and table from the data lake\n",
    "            similarities[i] = weighted_pairwise_cosine_similarity(embeddings_input, weights_input, embeddings_table, weights_table, i)\n",
    "        else:\n",
    "            similarities[i] = 0.0\n",
    "        # print(\"Similarity with Table {}: {}\".format(i,similarities[i]))\n",
    "\n",
    "        running_time += time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if k is None:\n",
    "        # If there is no k passed as an argument (k is the number of tables we want as an output (top-k most similar tables))\n",
    "        k = numTables # We rank all tables in the data lake\n",
    "    elif k > numTables:\n",
    "        print(\"The introduced k (k = {}) is larger than the number of tables in the data lake. Thus, the output is the ranking of all tables in the data lake.\".format(k))\n",
    "        k = numTables # We rank all tables in the data lake\n",
    "    \n",
    "    # Find indexes of the top-k tables sorted by similarity\n",
    "    ranked_tables_filtered = np.argsort(similarities)[::-1][:k]      \n",
    "    ranked_tables_filtered_similarities = np.sort(similarities)[::-1][:k] \n",
    "\n",
    "    running_time += time.time() - start_time   \n",
    "\n",
    "    return ranked_tables_filtered, ranked_tables_filtered_similarities, running_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
