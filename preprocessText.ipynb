{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepocess and clean the text\n",
    "def preprocess_text(x):\n",
    "    x = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', x) # Separate concatenated words marked by cappital letters\n",
    "    x = x.lower() # text to lowercase\n",
    "    x = re.sub(r'http\\S+', '',x) # remove URLs\n",
    "    x = re.sub(r'[^\\w]', ' ', x) # remove not alphanumeric symbols\n",
    "    x = re.sub(r'[0-9]', '',x) # remove numbers\n",
    "    x = x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))) # remove punctuation\n",
    "    x = nltk.word_tokenize(x) # tokenize\n",
    "    x = [token for token in x if token not in stop_words] # remove stop words\n",
    "    x = [lemmatizer.lemmatize(w) for w in x] # lemmatization\n",
    "    x = [word for word in x if len(word) > 2] # remove single characters\n",
    "    include_features = ['VB', 'NNP', 'NN'] # verbs, proper nouns and nouns (we think this is what makes the difference between topics. Adjectives, for instance, would be interesting for sentiment analysis of these topics, but not for topic modeling)\n",
    "    pos_tagged = nltk.pos_tag(x) # tag words\n",
    "    x = [text for text, pos in pos_tagged if pos in include_features] # select only the words tagged as the type we are interested in\n",
    "    x = ' '.join(x) # join all lemmatized words in one single text variable\n",
    "    return x\n",
    "\n",
    "stop_words = stopwords.words('english') # Define stop words\n",
    "new_sw = ['none', 'nan', 'unnamed']\n",
    "stop_words.extend(new_sw) # Extend stop words list with custom words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
